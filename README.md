# <img src="omlab.png" alt="speed" width=40> OmModel  

<p align="right">
<a href="README.md">English</a> | <a href="README_ZH.md">ä¸­æ–‡</a>
</p>

A collection of strong multimodal models for building the best multimodal agents

---

## ğŸ—“ï¸ Updates
* **07/04/2024**: [OmAgent](https://github.com/om-ai-lab/OmAgent) is now open-sourced. ğŸŒŸ Dive into our Multi-modal Agent Framework for complex video understanding. Read more in our [paper](https://arxiv.org/abs/2406.16620).
* **06/09/2024**: [OmChat](https://github.com/om-ai-lab/OmChat) has been released. ğŸ‰ Discover the capabilities of our multimodal language models, featuring robust video understanding and support for context up to 512k. More details in the [technical report]().
* **03/12/2024**: [OmDet](https://github.com/om-ai-lab/OmDet) is now open-sourced. ğŸš€ Experience our fast and accurate Open Vocabulary Detection (OVD) model, achieving 100 FPS. Learn more in our [paper](https://arxiv.org/abs/2209.05946).


---

## ğŸ—ƒï¸ Projects

Here are the various projects we've worked on at OmLab:

### â­ï¸ [OmAgent](https://github.com/om-ai-lab/OmAgent)
**Multi-modal Agent Framework for Complex Video Understanding with Task Divide-and-Conquer**

<img src="OmAgent.png" alt="speed" width=600>

### â­ï¸ [OmDet](https://github.com/om-ai-lab/OmDet)
**Fast and accurate open-vocabulary end-to-end object detection** 

<img src="turbo_model_structure.jpeg" alt="Image 1" width=600>


### â­ï¸ [OmChat](https://github.com/om-ai-lab/OmChat)
**Multimodal Language Models with Strong Long Context and Video Understanding**

<img src="omchat_structure.jpg" alt="speed" width=600>

### â­ï¸ [OVDEval](https://github.com/om-ai-lab/OVDEval)
**A Comprehensive Evaluation Benchmark for Open-Vocabulary Detection**

<img src="ovdeval_radar.jpg" alt="speed" width=600>

---

## ğŸ“œ Papers

Here are the research papers published by OmLab:

### ğŸ·ï¸ [How to Evaluate the Generalization of Detection? A Benchmark for Comprehensive Open-Vocabulary Detection](https://ojs.aaai.org/index.php/AAAI/article/view/28485/28945)
**Published in:** AAAI, 2024

### ğŸ·ï¸ [OmDet: Large-scale vision-language multi-dataset pre-training with multimodal detection network](https://arxiv.org/abs/2209.05946)
**Published in:** IET Computer Vision, 2024  

### ğŸ·ï¸ [Real-time Transformer-based Open-Vocabulary Detection with Efficient Fusion Head](https://arxiv.org/abs/2403.06892)
**Published in:** Arxiv. 2024  

### ğŸ·ï¸ [OmAgent: A Multi-modal Agent Framework for Complex Video Understanding with Task Divide-and-Conquer](https://arxiv.org/abs/2406.16620)
**Published in:** Arxiv. 2024  

### ğŸ·ï¸ [OmChat: A Recipe to Train Multimodal Language Models with Strong Long Context and Video Understanding]()
**Published in:** Arxiv. 2024 (Comming Soon)

### ğŸ·ï¸ [Sparta: Efficient open-domain question answering via sparse transformer matching retrieval](https://arxiv.org/pdf/2009.13013)
**Published in:** NAACL, 2021


---

## ğŸ“¬ Contact

For more information, feel free to reach out to us at [tianchez@hzlh.com](mailto:tianchez@hzlh.com).

---

Thank you for visiting OmModel's repository. We hope you find our projects and papers insightful and useful!
